{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fcwt\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from vit_pytorch.vit_3d import ViT\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "x = pickle.load(open('data_preprocessed_python/s01.dat', 'rb'), encoding='latin1')\n",
    "data = x['data']\n",
    "labels = x['labels']\n",
    "\n",
    "relevant_channels = data[:, :32, :]\n",
    "relevant_labels = labels[:, :2]\n",
    "\n",
    "classes = []\n",
    "for trial in range(40):\n",
    "    # 4 valence-arousal classes\n",
    "    valence, arousal = relevant_labels[trial][0], relevant_labels[trial][1]\n",
    "    cls = 0 if valence < 4.5 and arousal < 4.5 else \\\n",
    "                    1 if valence < 4.5 else \\\n",
    "                    2 if arousal < 4.5 else 3\n",
    "    classes.append(cls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize constant variables\n",
    "# paramaters for calculating cwt, not to be changed\n",
    "fs = 128 \n",
    "f0 = 4 # lowest frequency\n",
    "f1 = 45 # highest frequency\n",
    "fn = 32 # number of frequencies, match channel number for square frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_cwt = np.zeros((1024, 8064))\n",
    "\n",
    "for channel in range(32):    \n",
    "    signal = relevant_channels[0][channel]\n",
    "    _, current_cwt = fcwt.cwt(signal, fs, f0, f1, fn)\n",
    "    start = channel * fn\n",
    "    end = (channel + 1) * fn\n",
    "    total_cwt[start:end, :] = abs(current_cwt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame shape: (3, 32, 32)\n"
     ]
    }
   ],
   "source": [
    "# convert 2D time sample x channel-frequency format (x, y) to 3D channel x frequency x time format (x, y, z)\n",
    "# each 'frame' in all_frames is a 2D image showing each channel's CWT value for that time sample\n",
    "all_frames = []\n",
    "\n",
    "for sample in range(8064):\n",
    "    frame = np.zeros((32, 32))\n",
    "    for channel in range(32):\n",
    "        start = channel * fn\n",
    "        end = (channel + 1) * fn\n",
    "        frame[:, channel] = total_cwt[start:end, sample]\n",
    "\n",
    "    # normalize frame\n",
    "    norm_frame = (frame - frame.min()) / (frame.max() - frame.min())\n",
    "    # scale to 0-255\n",
    "    scaled_frame = (norm_frame * 255).astype(np.uint8)\n",
    "\n",
    "    # stack for RGB\n",
    "    frame_rgb = np.stack((scaled_frame,) * 3, axis=0)\n",
    "    frame_rgb = np.clip(frame_rgb, 0, 255).astype(np.uint8)\n",
    "\n",
    "    if sample == 0:\n",
    "        print(\"Frame shape:\", frame_rgb.shape)\n",
    "    all_frames.append(frame_rgb)\n",
    "    #all_frames.append(frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (8064, 3, 32, 32)\n"
     ]
    }
   ],
   "source": [
    "# write .gif file\n",
    "from array2gif import write_gif\n",
    "gif_dataset = np.array(all_frames)\n",
    "print(\"Dataset shape:\", gif_dataset.shape)\n",
    "write_gif(gif_dataset, 'test.gif', fps = 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split trial into 6-second samples as suggested in Arjun et al. 2021\n",
    "six_sec_samples = []\n",
    "sample_classes = []\n",
    "window_size = 6 * 128\n",
    "\n",
    "for i in range(0, 8064, window_size):\n",
    "    if i + window_size > 8064:\n",
    "        break\n",
    "    sample = all_frames[i:i+window_size]\n",
    "    six_sec_samples.append(sample)\n",
    "    sample_classes.append(classes[0]) # TODO: replace with correct index in final loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 768, 3, 32, 32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "array = np.array(six_sec_samples)\n",
    "array.shape\n",
    "# TODO: when looking at all trials ensure that shape is (10*40 samples, 768 frames, 32 height, 32 width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CWTDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = torch.tensor(np.array(data), dtype=torch.float32).permute(0, 2, 1, 3, 4)\n",
    "        self.data = self.data / 255.0\n",
    "        self.labels = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index], self.labels[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data, train_labels, test_labels = train_test_split(\n",
    "    six_sec_samples, sample_classes, test_size=0.2, random_state=23, stratify=sample_classes\n",
    ")\n",
    "train_loader = DataLoader(CWTDataset(train_data, train_labels, device=device), batch_size=4)\n",
    "test_loader = DataLoader(CWTDataset(test_data, test_labels, device=device), batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([4, 3, 768, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "test_sample, test_label = next(iter(train_loader))\n",
    "print(f\"Input shape: {test_sample.shape}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "vit = ViT(\n",
    "    image_size=32,\n",
    "    frames=768,\n",
    "    image_patch_size=4,\n",
    "    frame_patch_size=192,\n",
    "    num_classes=4,\n",
    "    dim=512,\n",
    "    depth=6,\n",
    "    heads=4,\n",
    "    mlp_dim=1024,\n",
    "    channels=3,\n",
    "    dropout=0.2,\n",
    "    emb_dropout=0.1,\n",
    "    pool='cls'\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 768, 3, 32, 32)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "\tEpoch: 0/50\tLoss:0.000\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTest accuracy: 100.00%\n"
     ]
    }
   ],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(vit.parameters(), lr=3e-4, weight_decay=0.1)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=50)\n",
    "\n",
    "print(\"Training...\")\n",
    "for epoch in range(10):\n",
    "    vit.train()\n",
    "    for inputs, targets in train_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = vit(inputs)\n",
    "        loss = loss_fn(outputs, targets)\n",
    "        print(f\"\\tEpoch: {epoch}/50\\tLoss:{loss:.3f}\", end='\\r')\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    scheduler.step()\n",
    "\n",
    "vit.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, targets in test_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        outputs = vit(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += targets.size(0)\n",
    "        correct += (predicted == targets).sum().item()\n",
    "accuracy = 100 * correct / total\n",
    "print(f\"\\tTest accuracy: {accuracy:.2f}%\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "emotion",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
